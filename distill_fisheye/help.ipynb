{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the NVIDIA Source Code License\n",
    "# ---------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "# from timm.models.registry import register_model\n",
    "# from timm.models.vision_transformer import _cfg\n",
    "# from mmseg.models.builder import BACKBONES\n",
    "# from mmseg.utils import get_root_logger\n",
    "# from mmcv.runner import load_checkpoint\n",
    "# import math\n",
    "\n",
    "def drop_path(input, drop_prob: float = 0.0, training: bool = False, scale_by_keep=True):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n",
    "    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n",
    "    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n",
    "    argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return input\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = input.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        return drop_path(hidden_states, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"p={}\".format(self.drop_prob)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         trunc_normal_(m.weight, std=.02)\n",
    "    #         if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "    #             nn.init.constant_(m.bias, 0)\n",
    "    #     elif isinstance(m, nn.LayerNorm):\n",
    "    #         nn.init.constant_(m.bias, 0)\n",
    "    #         nn.init.constant_(m.weight, 1.0)\n",
    "    #     elif isinstance(m, nn.Conv2d):\n",
    "    #         fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "    #         fan_out //= m.groups\n",
    "    #         m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "    #         if m.bias is not None:\n",
    "    #             m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         trunc_normal_(m.weight, std=.02)\n",
    "    #         if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "    #             nn.init.constant_(m.bias, 0)\n",
    "    #     elif isinstance(m, nn.LayerNorm):\n",
    "    #         nn.init.constant_(m.bias, 0)\n",
    "    #         nn.init.constant_(m.weight, 1.0)\n",
    "    #     elif isinstance(m, nn.Conv2d):\n",
    "    #         fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "    #         fan_out //= m.groups\n",
    "    #         m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "    #         if m.bias is not None:\n",
    "    #             m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            x_ = self.norm(x_)\n",
    "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        else:\n",
    "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         trunc_normal_(m.weight, std=.02)\n",
    "    #         if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "    #             nn.init.constant_(m.bias, 0)\n",
    "    #     elif isinstance(m, nn.LayerNorm):\n",
    "    #         nn.init.constant_(m.bias, 0)\n",
    "    #         nn.init.constant_(m.weight, 1.0)\n",
    "    #     elif isinstance(m, nn.Conv2d):\n",
    "    #         fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "    #         fan_out //= m.groups\n",
    "    #         m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "    #         if m.bias is not None:\n",
    "    #             m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
    "\n",
    "        return x\n",
    "\n",
    "def to_2tuple(x):\n",
    "    return (x,x)\n",
    "\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
    "        self.num_patches = self.H * self.W\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    #     self.apply(self._init_weights)\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         trunc_normal_(m.weight, std=.02)\n",
    "    #         if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "    #             nn.init.constant_(m.bias, 0)\n",
    "    #     elif isinstance(m, nn.LayerNorm):\n",
    "    #         nn.init.constant_(m.bias, 0)\n",
    "    #         nn.init.constant_(m.weight, 1.0)\n",
    "    #     elif isinstance(m, nn.Conv2d):\n",
    "    #         fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "    #         fan_out //= m.groups\n",
    "    #         m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "    #         if m.bias is not None:\n",
    "    #             m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, H, W\n",
    "\n",
    "\n",
    "class MixVisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=5, in_chans=3, num_classes=1000, embed_dims=[32, 64, 160, 256],\n",
    "                 num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0.1, norm_layer=nn.LayerNorm,\n",
    "                 depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1]):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "\n",
    "        # patch_embed\n",
    "        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n",
    "                                              embed_dim=embed_dims[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n",
    "                                              embed_dim=embed_dims[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n",
    "                                              embed_dim=embed_dims[2])\n",
    "        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n",
    "                                              embed_dim=embed_dims[3])\n",
    "\n",
    "        # transformer encoder\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "        cur = 0\n",
    "        self.block1 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[0])\n",
    "            for i in range(depths[0])])\n",
    "        self.norm1 = norm_layer(embed_dims[0])\n",
    "\n",
    "        cur += depths[0]\n",
    "        self.block2 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[1])\n",
    "            for i in range(depths[1])])\n",
    "        self.norm2 = norm_layer(embed_dims[1])\n",
    "\n",
    "        cur += depths[1]\n",
    "        self.block3 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[2])\n",
    "            for i in range(depths[2])])\n",
    "        self.norm3 = norm_layer(embed_dims[2])\n",
    "\n",
    "        cur += depths[2]\n",
    "        self.block4 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[3])\n",
    "            for i in range(depths[3])])\n",
    "        self.norm4 = norm_layer(embed_dims[3])\n",
    "\n",
    "        # classification head\n",
    "        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    #     self.apply(self._init_weights)\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         trunc_normal_(m.weight, std=.02)\n",
    "    #         if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "    #             nn.init.constant_(m.bias, 0)\n",
    "    #     elif isinstance(m, nn.LayerNorm):\n",
    "    #         nn.init.constant_(m.bias, 0)\n",
    "    #         nn.init.constant_(m.weight, 1.0)\n",
    "    #     elif isinstance(m, nn.Conv2d):\n",
    "    #         fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "    #         fan_out //= m.groups\n",
    "    #         m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "    #         if m.bias is not None:\n",
    "    #             m.bias.data.zero_()\n",
    "\n",
    "    # def init_weights(self, pretrained=None):\n",
    "    #     if isinstance(pretrained, str):\n",
    "    #         logger = get_root_logger()\n",
    "    #         load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n",
    "\n",
    "    # def reset_drop_path(self, drop_path_rate):\n",
    "    #     dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n",
    "    #     cur = 0\n",
    "    #     for i in range(self.depths[0]):\n",
    "    #         self.block1[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    #     cur += self.depths[0]\n",
    "    #     for i in range(self.depths[1]):\n",
    "    #         self.block2[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    #     cur += self.depths[1]\n",
    "    #     for i in range(self.depths[2]):\n",
    "    #         self.block3[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    #     cur += self.depths[2]\n",
    "    #     for i in range(self.depths[3]):\n",
    "    #         self.block4[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    # def freeze_patch_emb(self):\n",
    "    #     self.patch_embed1.requires_grad = False\n",
    "\n",
    "    # @torch.jit.ignore\n",
    "    # def no_weight_decay(self):\n",
    "    #     return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n",
    "\n",
    "    # def get_classifier(self):\n",
    "    #     return self.head\n",
    "\n",
    "    # def reset_classifier(self, num_classes, global_pool=''):\n",
    "    #     self.num_classes = num_classes\n",
    "    #     self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        for i, blk in enumerate(self.block1):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for i, blk in enumerate(self.block2):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for i, blk in enumerate(self.block3):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        for i, blk in enumerate(self.block4):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        # x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the NVIDIA Source Code License\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "class head_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, embedding_dim=2048):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=embedding_dim*4,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(embedding_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(self.bn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# @HEADS.register_module()\n",
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels=[32, 64, 160, 256],embed_dim=256,num_class=19):\n",
    "        super(SegFormerHead, self).__init__()\n",
    "        # assert len(feature_strides) == len(self.in_channels)\n",
    "        # assert min(feature_strides) == feature_strides[0]\n",
    "        # self.feature_strides = feature_strides\n",
    "        self.num_classes = num_class\n",
    "        self.in_channels = in_channels\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        # decoder_params = kwargs['decoder_params']\n",
    "        embedding_dim = embed_dim#decoder_params['embed_dim']\n",
    "\n",
    "        self.linear_c4 = head_MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c3 = head_MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c2 = head_MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c1 = head_MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n",
    "\n",
    "        self.linear_fuse =ConvModule(\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "            # norm_cfg=dict(type='SyncBN', requires_grad=True)\n",
    "        # self.batch_norm = nn.BatchNorm2d(embedding_dim)\n",
    "        # self.activation = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n",
    "        c1, c2, c3, c4 = x\n",
    "\n",
    "        ############## MLP decoder on C1-C4 ###########\n",
    "        n, _, h, w = c4.shape\n",
    "\n",
    "        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "        _c4 = nn.functional.interpolate(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "        _c3 = nn.functional.interpolate(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
    "        _c2 = nn.functional.interpolate(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "\n",
    "        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "\n",
    "        return x,_c\n",
    "\n",
    "class Segformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = MixVisionTransformer()\n",
    "        self.decode_head = SegFormerHead()\n",
    "    def forward(self,x):\n",
    "        x=self.backbone(x)\n",
    "        x,fused_layer=self.decode_head(x)\n",
    "        return x,fused_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import tkinter as tk\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "class fisheye(object):\n",
    "    def __init__(self) -> None:\n",
    "        # self.img = None \n",
    "        # self.fimg = None\n",
    "        # self.map = None\n",
    "        \n",
    "        para = []\n",
    "        \n",
    "        self.pitch = -0.4\n",
    "        self.yaw=0\n",
    "        self.trans_x=1\n",
    "        self.trans_y=1\n",
    "        self.f0 = 700\n",
    "        self.zoom = 0.5\n",
    "        self.position_y = 1\n",
    "        self.position_x = 1\n",
    "        self.size_x = 256\n",
    "        self.size_y = 256\n",
    "        self.model=1\n",
    "\n",
    "    def norm2fisheye(self,img,label=False):\n",
    "        # self.img=img\n",
    "        map_x,map_y = self.model1(img,size=img.shape,label=label)\n",
    "        return map_x,map_y\n",
    "\n",
    "\n",
    "    def model1(self,img,size=(512,512,3),label=False):\n",
    "        # Orthographic\n",
    "        w,h,C = size\n",
    "        self.size_x,self.size_y = w/2,h/2\n",
    "        # w,h=self.img.shape[1],self.img.shape[0]\n",
    "        f0 = w/512*self.f0 #f0 gets bigger, distortion gets smaller\n",
    "        fc = int(self.zoom*w/np.sin(np.arctan(w/(2*f0))))\n",
    "        # img= self.img\n",
    "        trans_x = self.trans_x\n",
    "        trans_y = self.trans_y\n",
    "        yaw = self.yaw\n",
    "        pitch = self.pitch #pitch angle of fisheye camera\n",
    "        # fc = self.fc #fisheye focal length\n",
    "        rx = int(self.size_x) #image size\n",
    "        ry = int(self.size_y)  \n",
    "        ##build the transform map\n",
    "        u=np.linspace(0,2*rx,2*rx)\n",
    "        v=np.linspace(0,2*ry,2*ry)\n",
    "        udst,vdst = np.meshgrid(u,v)\n",
    "        v,u = vdst-self.position_y*ry+fc*np.sin(pitch)+450*fc*(1-trans_y)/f0 ,\\\n",
    "            udst-self.position_x*rx+fc*np.sin(yaw)+250*w/512*fc*(1-trans_x)/f0 #get proxy\n",
    "        \n",
    "        # rotate the fisheye sphere\n",
    "        r1 = np.sqrt(fc**2-u**2)\n",
    "        yc = r1*np.sin(np.arcsin(v/r1)-pitch)\n",
    "\n",
    "        r1 = np.sqrt(fc**2-yc**2)\n",
    "        filter2 = np.arcsin(u/r1)-yaw\n",
    "        # filter2[filter2>np.pi/2]=None\n",
    "        # filter2[filter2<-np.pi/2]=None\n",
    "        xc = r1*np.sin(filter2) \n",
    "\n",
    "        # convert the proxy into raw image\n",
    "        r = np.sqrt(xc**2+yc**2)\n",
    "        r0 = f0*np.tan(np.arcsin(r/fc))\n",
    "        p_theta = np.arctan2(yc,xc)\n",
    "        x,y = r0*np.cos(p_theta),r0*np.sin(p_theta)\n",
    "\n",
    "        map_x = x+w/2*trans_x\n",
    "        map_y = y+h/2*trans_y\n",
    "        map_y = np.array(map_y,dtype=np.float32)\n",
    "        map_x = np.array(map_x,dtype=np.float32)\n",
    "        # self.map_x = map_x\n",
    "        # self.map_y = map_y\n",
    "        #transform\n",
    "        return map_x,map_y\n",
    "        # if label==True:\n",
    "        #     return #cv2.remap(img,map_x,map_y,cv2.INTER_NEAREST,borderValue=255)\n",
    "        # else:return  #cv2.remap(img,map_x,map_y,cv2.INTER_LINEAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pr,gt,eps=1e-7):\n",
    "    IoUs = {}\n",
    "    acc= {}\n",
    "    sum_iou = 0\n",
    "    sum_acc = 0\n",
    "    for i in range(1,pr.size()[1]):\n",
    "        if torch.sum(gt[:, i, :, :]) == 0 :\n",
    "            # remove the unlabeled channel\n",
    "            pass\n",
    "        else: \n",
    "            intersection = torch.sum(gt[:,i,:,:] * pr[:,i,:,:])\n",
    "            union = torch.sum(gt[:,i,:,:]) + torch.sum(pr[:,i,:,:]) - intersection + eps\n",
    "            IoUs[i] = (intersection + eps) / union\n",
    "            acc[i] =  (intersection + eps) / (torch.sum(gt[:,i,:,:])+eps)\n",
    "            sum_iou += IoUs[i]\n",
    "            sum_acc += acc[i]\n",
    "            # IoUs.append((intersection + eps) / union)\n",
    "            # acc.append((intersection + eps) / (torch.sum(gt[:,i,:,:])+eps))\n",
    "    mIoUs = sum_iou/len(IoUs)\n",
    "    macc = sum_acc/len(acc)\n",
    "    \n",
    "    return {\"miou\":mIoUs,\"macc\":macc,'per_iou':IoUs,'per_acc':acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "# from PIL import Image\n",
    "from transformers import SegformerFeatureExtractor\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "from transformers import (SegformerForSemanticSegmentation,\n",
    "        SegformerModel,\n",
    "        SegformerDecodeHead,\n",
    "        get_constant_schedule_with_warmup,\n",
    "        get_linear_schedule_with_warmup,\n",
    "        get_cosine_with_hard_restarts_schedule_with_warmup)\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "# import PIL\n",
    "from torchvision.datasets import Cityscapes\n",
    "# from norm2fisheye import fisheye\n",
    "import torch.nn.functional as F\n",
    "# from meaniou import meanIOU\n",
    "# from metrics import calculate_metrics\n",
    "# from .model.segformer import Segformer\n",
    "\n",
    "class mydataset(Cityscapes):\n",
    "    def __init__(self,root,split,mode,target_type,test_mode=False):\n",
    "        super().__init__(root,split,mode,target_type)\n",
    "        # self.feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\")\n",
    "        self.test_mode = test_mode\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self.class_map = dict(zip(self.valid_classes, range(len(self.valid_classes))))\n",
    "\n",
    "    def encode_segmap(self,mask):\n",
    "        for _voidc in self.void_classes:\n",
    "            mask[mask == _voidc] = 255\n",
    "        for _validc in self.valid_classes:\n",
    "            mask[mask == _validc] = self.class_map[_validc]\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image = cv2.imread(self.images[idx])\n",
    "        target = cv2.imread(self.targets[idx][0])\n",
    "        image = cv2.resize(image,dsize=(512,512))\n",
    "        target = cv2.resize(target,dsize=(512,512))\n",
    "        print(target.shape)\n",
    "        target = self.encode_segmap(target)[:,:,0]\n",
    "        sys.exit()\n",
    "        fi = fisheye()\n",
    "        if self.test_mode == False:\n",
    "            fi.f0 = np.random.randint(200,600)\n",
    "            fi.pitch = np.random.uniform(-0.8,0)\n",
    "            fi.trans_x = np.random.uniform(0.5,1.5)\n",
    "        map_x,map_y = fi.norm2fisheye(image)\n",
    "        # target = fi.norm2fisheye(target,label=True)\n",
    "        inputs = self.feature_extractor(image,target,return_tensors='pt')\n",
    "        # ori_inputs =self.feature_extractor(image,return_tensors='pt')\n",
    "        for k,v in inputs.items():\n",
    "            inputs[k].squeeze_()\n",
    "        # ori_inputs['pixel_values'].squeeze_()\n",
    "\n",
    "        return inputs,map_x,map_y\n",
    "\n",
    "def convert2fisheye(imgs,grid,label=False):\n",
    "    if label==False:\n",
    "        outp = F.grid_sample(imgs.permute(0,3,1,2),grid=grid,mode='bilinear')\n",
    "    else:\n",
    "        outp = F.grid_sample(imgs.unsqueeze(dim=1),grid=grid,mode='nearest')\n",
    "    return outp\n",
    "\n",
    "def mse_loss(input, target, ignored_index=255, reduction='mean'):\n",
    "    mask = target == ignored_index\n",
    "    out = (input1[~mask]-target[~mask])**2\n",
    "    if reduction == \"mean\":\n",
    "        return out.mean()\n",
    "    elif reduction == \"None\":\n",
    "        return out\n",
    "\n",
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, learning_rate= 2e-4,train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        # self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\")\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.learning_rate=learning_rate\n",
    "        self.num_classes = 19 #len(id2label.keys())\n",
    "        # self.MSEloss = mse_loss#nn.MSELoss()\n",
    "        self.CEloss = nn.CrossEntropyLoss(ignore_index=255)\n",
    "        self.t_model = Segformer()\n",
    "        for param in self.t_model.named_parameters():\n",
    "            param[1].requires_grad=False\n",
    "        self.s_model = Segformer()\n",
    "            # return_dict=False,\n",
    "        # pretrained_dict = torch.load(\"/mnt/ssd/home/tianxiaofeng/segformer_train/lightning_logs/version_26/checkpoints/epoch=9-step=2050.ckpt\")['state_dict']\n",
    "        # pretrained_dict = {key.replace('model.',''): value for key,value in pretrained_dict.items()}\n",
    "        # self.model.load_state_dict(pretrained_dict)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # images,masks,map_x,map_y = batch[0]['pixel_values'],batch[1]['pixel_values'], batch[1]['labels'],batch[2],batch[3]\n",
    "        images,masks,map_x,map_y = batch[0]['pixel_values'],batch[0]['labels'],batch[1],batch[2]\n",
    "        B,w,h,C = images.shape\n",
    "        map_x = [torch.tensor(i).float().to(images.device) for i in map_x]\n",
    "        map_y = [torch.tensor(i).float().to(images.device) for i in map_y]\n",
    "        map_x = torch.stack(map_x,dim=0)\n",
    "        map_y = torch.stack(map_y,dim=0)\n",
    "        grid = torch.stack((map_x/((w)/2)-1,map_y/((h)/2)-1),dim=3)#.unsqueeze(0)\n",
    "\n",
    "        fimages = convert2fisheye(imgs=images.permute(0,2,3,1),grid=grid)\n",
    "        masks = convert2fisheye(imgs=masks-255,grid=grid,label=True)\n",
    "        masks = masks.squeeze(dim=1)+255 #padding with 255\n",
    "\n",
    "        # print(masks.shape)\n",
    "        # f_inputs = feature_extractor(fimages,return_tensors='pt')\n",
    "        # ori_inputs = feature_extractor(images,return_tensors='pt')\n",
    "        \n",
    "        _,t_hidden = self.t_model(images)\n",
    "        # s_outputs = self.s_model(pixel_values=fimages,labels=masks)\n",
    "        s_outputs,s_hidden = self.s_model(fimages)\n",
    "        \n",
    "        t_hidden = nn.functional.interpolate(\n",
    "            t_hidden, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        t_hidden = F.grid_sample(t_hidden.permute(0,3,1,2),grid=grid,mode='bilinear')\n",
    "        \n",
    "        t_hidden = nn.functional.interpolate(\n",
    "            t_hidden, \n",
    "            size=s_hidden.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        CEloss = self.CEloss(s_outputs,masks)\n",
    "        loss= 0.7*CEloss + 0.3*mse_loss(s_hidden,t_hidden,ignore_index=0)\n",
    "        \n",
    "        \n",
    "        return({'loss': loss})\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log('loss',avg_train_loss,on_epoch=True,on_step=False)\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        images,masks,map_x,map_y = batch[0]['pixel_values'],batch[0]['labels'],batch[1],batch[2]\n",
    "        B,w,h,C = images.shape\n",
    "        map_x = [torch.tensor(i).float().to(images.device) for i in map_x]\n",
    "        map_y = [torch.tensor(i).float().to(images.device) for i in map_y]\n",
    "        map_x = torch.stack(map_x,dim=0)\n",
    "        map_y = torch.stack(map_y,dim=0)\n",
    "        grid = torch.stack((map_x/((w)/2)-1,map_y/((h)/2)-1),dim=3)#.unsqueeze(0)\n",
    "\n",
    "        fimages = convert2fisheye(imgs=images.permute(0,2,3,1),grid=grid)\n",
    "        masks = convert2fisheye(imgs=masks-255,grid=grid,label=True)\n",
    "        masks = masks.squeeze(dim=1)+255 #padding with 255\n",
    "        \n",
    "        # print(masks.shape)\n",
    "        # f_inputs = feature_extractor(fimages,return_tensors='pt')\n",
    "        # ori_inputs = feature_extractor(images,return_tensors='pt')\n",
    "        \n",
    "        _,t_hidden = self.t_model(images)\n",
    "        # s_outputs = self.s_model(pixel_values=fimages,labels=masks)\n",
    "        s_outputs,s_hidden = self.s_model(fimages)\n",
    "        \n",
    "        t_hidden = nn.functional.interpolate(\n",
    "            t_hidden, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        t_hidden = F.grid_sample(t_hidden.permute(0,3,1,2),grid=grid,mode='bilinear')\n",
    "        \n",
    "        t_hidden = nn.functional.interpolate(\n",
    "            t_hidden, \n",
    "            size=s_hidden.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        CEloss = self.CEloss(s_outputs,masks)\n",
    "        loss= 0.7*CEloss + 0.3*mse_loss(s_hidden,t_hidden,ignore_index=0)\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            s_outputs, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        masks = F.one_hot(masks,num_classes=19).permute(0,3,1,2)\n",
    "        # print(masks.shape)\n",
    "        predicted = output.argmax(dim=1)\n",
    "        predicted = F.one_hot(predicted,num_classes=19).permute(0,3,1,2)\n",
    "        cm = {}\n",
    "        cm = calculate_metrics(predicted,masks)\n",
    "        return({'val_loss': loss,'val_miou':cm['miou'],'mean_acc':cm['macc']})\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_val_miou = torch.stack([x[\"val_miou\"] for x in outputs]).mean()\n",
    "        avg_val_macc =  torch.stack([x[\"mean_acc\"] for x in outputs]).mean()\n",
    "        metrics = {\"val_loss\": avg_val_loss, \"val_mean_iou\":avg_val_miou,\"val_mean_acc\":avg_val_macc}\n",
    "        \n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v)\n",
    "        return ({'avg_val_loss':avg_val_loss})\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.test_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "            \n",
    "        return({'test_loss': loss})\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        metrics = self.test_mean_iou.compute(\n",
    "              num_labels=self.num_classes, \n",
    "              ignore_index=0, \n",
    "              reduce_labels=False,\n",
    "          )\n",
    "       \n",
    "        avg_test_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        test_mean_iou = metrics[\"mean_iou\"]\n",
    "        test_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "\n",
    "        metrics = {\"test_loss\": avg_test_loss, \"test_mean_iou\":test_mean_iou, \"test_mean_accuracy\":test_mean_accuracy}\n",
    "        \n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(filter(lambda p:p.requires_grad,self.parameters()),lr=self.learning_rate)\n",
    "        # scheduler = get_linear_schedule_with_warmup(\n",
    "        #     optimizer,\n",
    "        #     num_warmup_steps=0,\n",
    "        #     num_training_steps=8000\n",
    "        # )\n",
    "        scheduler = get_constant_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=100,\n",
    "        )\n",
    "        scheduler = {\"scheduler\":scheduler, \"interval\":\"step\",\"frequency\":1}\n",
    "        \n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\")\n",
    "    # print(dir(feature_extractor))\n",
    "    # feature_extractor.reduce_labels = False\n",
    "    # feature_extractor.size = 128\n",
    "    train_dataset = mydataset(\n",
    "    '/mnt/hdd/dataset/cityscapes/extracted/',\n",
    "                split='train',\n",
    "                mode='fine',\n",
    "                target_type='semantic'                \n",
    "    )\n",
    "\n",
    "    val_dataset = mydataset(\n",
    "        '/mnt/hdd/dataset/cityscapes/extracted/',\n",
    "                    split='val',\n",
    "                    mode='fine',\n",
    "                    target_type='semantic' ,\n",
    "                    test_mode=True               \n",
    "    )\n",
    "    test_dataset = mydataset(\n",
    "        '/mnt/hdd/dataset/cityscapes/extracted/',\n",
    "                    split='test',\n",
    "                    mode='fine',\n",
    "                    target_type='semantic' ,\n",
    "                    test_mode=True               \n",
    "    )\n",
    "\n",
    "    batch_size = 10\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=5)#pin_memory=True, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size,num_workers=5)#pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size,num_workers=5)#,pin_memory=True)#,num_workers=3,prefetch_factor=8\n",
    "\n",
    "    # segformer_finetuner=SegformerFinetuner.load_from_checkpoint(\n",
    "    #     \"/mnt/ssd/home/tianxiaofeng/segformer_train/lightning_logs/version_24/checkpoints/epoch=6-step=1250.ckpt\"\n",
    "    # )\n",
    "    segformer_finetuner = SegformerFinetuner(\n",
    "        train_dataloader=train_dataloader, \n",
    "        val_dataloader=val_dataloader,\n",
    "        test_dataloader=test_dataloader\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor()\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"avg_val_loss\", \n",
    "        min_delta=0.00, \n",
    "        patience=7, \n",
    "        verbose=False, \n",
    "        mode=\"min\",\n",
    "    )   \n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                        save_top_k=-1,\n",
    "                        save_weights_only=True,\n",
    "                        every_n_train_steps=750,\n",
    "                        )\n",
    "                        # save_on_train_epoch_end=True,\n",
    "                        # save_last=True,\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=[2],\n",
    "        callbacks=[lr_monitor, checkpoint_callback],\n",
    "        max_epochs=100,\n",
    "        check_val_every_n_epoch=10)\n",
    "        # log_every_n_steps=20,\n",
    "    #     resume_from_checkpoint=\"/mnt/ssd/home/tianxiaofeng/segformer_train/lightning_logs/version_24/checkpoints/epoch=6-step=1250.ckpt\"\n",
    "    # )\n",
    "    # trainer.tune(segformer_finetuner)\n",
    "    # print(segformer.learning_rate)\n",
    "    trainer.fit(segformer_finetuner)\n",
    "            #   ckpt_path=\"/mnt/ssd/home/tianxiaofeng/distill_fisheye/lightning_logs/version_3/checkpoints/epoch=99-step=7500.ckpt\")\n",
    "# function ConnectButton(){\n",
    "#     console.log(\"Connect pushed\"); \n",
    "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "# }\n",
    "\n",
    "# Interval(ConnectButton,60000);\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
